---
title: "Text as Data: Problem Set 01"
author: "Emily Zhao"
execute: 
  error: true
format:
  html
editor_options: 
  chunk_output_type: console
theme: default
---

```{r setup, include=FALSE}
# Load packages via pacman
if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
pacman::p_load(
  dplyr, stringr, readr, tidyr, purrr,    # core tidy tools
  quanteda, quanteda.textstats, ggwordcloud, tidytext
)

# Read data
data_path <- "/Users/zhaoshuti/Desktop/TAT/problem-set-01-2025-ShutiZhao/midterm_candidates_labeled_all_May05.csv" 
df <- readr::read_csv(data_path, guess_max = 1e6)

# Debug: Check if data loaded
cat("Data loaded successfully!\n")
cat("Number of rows:", nrow(df), "\n")
cat("Column names:", paste(names(df), collapse = ", "), "\n")
```



## Instructions

To complete this homework, you have two options. 

You can either complete this assignment using a **dataset of your choice**. This is a great opportunity for you to start working with a dataset that you can potentially use for your final project. 

Your second option is to this dataset of social media posts from political candidates running for Congress during the 2022 U.S. Midterm election. 

You can download the dataset [here](https://www.dropbox.com/scl/fi/jz14130dlzj5xvca9txj4/midterm_candidates_labeled_all_May05.csv?rlkey=b0m1h6kk1af3uz9tjqyelthh3&dl=0)

To know more about the dataset, read and cite this paper: <https://journals.sagepub.com/doi/full/10.1177/20563051251337541>

If you want say thanks to the people who collected and shared this data with us for this class, please send a email to my colleague [Maggie MacDonald](https://maggiegmacdonald.com/)

**IMPORTANT:** Remember to NOT commit your data to github. Github does not allow you to push large datasets to remote repositories. If you commit your dataset, you will need to reset your commit, and that's always a bit of work. In addition, remember that your notebook should be compiled with the results of the code blocks.

## Question 1 

Take a small a sample of your documents and read them carefully. This sample doesn't need to be random. Select cases that are more interesting, and close to your theoretical interest in this dataset. What are you thoughts about these documents? What did you learn after reading them?

```{r question1, echo=TRUE, results='markup'}
terms <- c("inflation","economy","jobs","crime","abortion","immigration",
           "border","climate","guns","healthcare","tax","education")

df %>%
  dplyr::filter(!is.na(text)) %>%
  dplyr::filter(stringr::str_detect(stringr::str_to_lower(text),
                                    stringr::str_c(terms, collapse = "|"))) %>%
  dplyr::slice_sample(n = 30) %>%
  dplyr::select(party_clean, created_at, username, text)
```

```{r}
# First, create the sample data
terms <- c("inflation","economy","jobs","crime","abortion","immigration",
           "border","climate","guns","healthcare","tax","education")

df_sample <- df %>%
  dplyr::filter(!is.na(text)) %>%
  dplyr::filter(stringr::str_detect(stringr::str_to_lower(text),
                                    stringr::str_c(terms, collapse = "|"))) %>%
  dplyr::slice_sample(n = 30)

# Define column names
text_col <- "text"
date_col <- "created_at"
party_col <- "party_clean"
author_col <- "username"

# Now create the readable format
df_to_read <- df_sample |>
  dplyr::transmute(
    party = .data[[party_col]], 
    date = as.character(.data[[date_col]]), 
    author = .data[[author_col]],
    text_full = stringr::str_squish(.data[[text_col]]),
    text_short = dplyr::if_else(nchar(text_full) > 280,
                                paste0(substr(text_full, 1, 277), "..."),
                                text_full)
  )

# Print the short version in a table
knitr::kable(df_to_read[, c("party","date","author","text_short")], format = "html")

# List full texts below
cat(paste0("• ", df_to_read$text_full), sep = "\n\n")
```

**thoughts and observations:**

When I looked through a small sample of posts that focused on policy issues like inflation, jobs, crime, abortion, immigration, and education, a few things stood out to me.

The first is how differently the same topics are framed depending on the candidate. Posts about inflation, for example, sometimes talk about everyday struggles like the rising cost of groceries or Thanksgiving dinner, while others use the issue to criticize government spending and tie it to partisan blame. Similarly, on abortion and healthcare, some candidates emphasize rights and protections, while others stress morality or security. Reading them side by side makes it clear that even when everyone is talking about the same problem, they present it in very different ways.

Another thing I noticed is how much of the content is “campaign talk.” Many posts combine an issue with a call to action—come to a rally, donate, vote early, or volunteer. They’re filled with hashtags, @mentions, links, and emojis, which makes sense for social media but also means a lot of this text will have to be cleaned or standardized before analysis. At the same time, some hashtags are clearly issue signals (like #ProChoice or #SaveAmerica), so I might want to keep those instead of stripping them all away.

Overall, reading these posts helped me see both the main themes and the style of communication. Candidates are trying to link national debates to everyday life, while also pushing people to engage with their campaigns. It gave me a clearer sense of what matters in preprocessing—removing clutter, but also keeping the pieces that carry real political meaning.

## Question 2 

Tokenize your documents and pre-process them, removing any "extraneous" content you noticed in closely reading a sample of your documents. What content have you removed and why? Any pre-processing steps from what you saw in class that you consider not to use here? 

```{r}
txt <- tolower(as.character(df$text))

# remove obvious junk, keep it simple
txt <- str_replace_all(txt, "https?://\\S+|www\\.\\S+", " ")  # URLs
txt <- str_replace_all(txt, "@\\w+", " ")                     # @handles
txt <- str_replace_all(txt, "&amp;|&lt;|&gt;", " ")           # HTML bits
txt <- str_replace_all(txt, "#", "")                          # drop '#' but keep hashtag words
txt <- str_replace_all(txt, "[^a-z\\s]", " ")                 # emojis/punct/numbers → space
txt <- str_squish(txt)

# tokenize -> remove stopwords -> dfm -> light trim
corp   <- corpus(txt)
toks   <- tokens(corp, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE) |>
          tokens_remove(stopwords("en"))
dfm_q2 <- dfm(toks) |> dfm_trim(min_termfreq = 5)

# tiny peek
head(textstat_frequency(dfm_q2, n = 20))
```

**thoughts and observations:**
I cleaned the text by lowercasing and removing obvious platform noise: URLs, @mentions, HTML artifacts (&amp;, &gt;, etc.), punctuation/numbers/emojis, and extra whitespace. I kept hashtag words for topic signal (e.g., #Inflation → inflation) but dropped the #.

Then I tokenized, removed English stopwords, built a document-feature matrix, and lightly trimmed very rare terms (min_termfreq = 5) to reduce one-off noise. A quick peek at top terms after cleaning looked reasonable and content-bearing.

I intentionally did not use stemming/lemmatization (to keep words readable), didn’t build n-grams, and didn’t apply TF-IDF at this stage—the goal here was just a clear, simple token set for downstream analysis.

## Question 3

Using [Danny and Spirling's Pretext](https://github.com/matthewjdenny/preText), tell me which of the pre-processing steps makes a more substantive difference in the data transformation? Would you keep all these steps or would you follow the PreText recommendations? Use a sample of the data to solve this question. 

```{r}
install.packages(c("remotes", "BiocManager"))  # BiocManager is harmless; some deps check it
remotes::install_github("matthewjdenny/preText", build_vignettes = FALSE, dependencies = TRUE)
library(preText)
```

```{r}
# Build a small, clean sample (preText can be slow on 100k+ docs)
set.seed(123)
q3_sample <- df %>%
  filter(!is.na(text), nchar(text) > 5) %>%
  slice_sample(n = 1500) %>%               
  pull(text) %>%
  as.character()
```
```{r}
# Run factorial preprocessing across many schemes
# ngrams off to save time
pre_docs <- preText::factorial_preprocessing(
  q3_sample,
  use_ngrams = FALSE,
  infrequent_term_threshold = 0.01,          # drop terms in <1% of docs within each scheme
  verbose = TRUE
)
```
```{r}
# Compute preText scores (how much each scheme changes doc distances)
pt_res <- preText::preText(
  pre_docs,
  dataset_name    = "Midterm Candidates (sample)",
  distance_method = "cosine",
  num_comparisons = 50,                      # use 50 for saving time
  verbose = TRUE
)
```
```{r}
# Plots: scores by scheme + marginal effects of each step
preText::preText_score_plot(pt_res)
preText::regression_coefficient_plot(pt_res, remove_intercept = TRUE)
```

**Your thoughts and observations:**
PreText shows that the only preprocessing choice that materially changes the document representation is removing infrequent terms—it has the largest positive coefficient, meaning it alters distances between documents the most. In contrast, lowercasing, removing numbers, and removing stopwords have very small effects on the data, and stemming and removing punctuation slightly stabilize results (negative coefficients), likely by merging near-duplicate tokens. Practically, this means the model is sensitive to how aggressively you trim rare words; over-trimming can erase distinctive but informative tokens common in social media (unique hashtags, names, local issues). I would therefore keep light normalization (lowercase, stopwords, punctuation) and tune rare-term trimming conservatively (e.g., a lower threshold or an absolute count), using stemming only if you’re comfortable trading a bit of interpretability for stability.


## Question 4 

Pick an important source of variation in your data (for example: date, author identity, location, etc.). Subset the data along this dimension, and discuss which words discriminate better each group. You can do this by using TF-IDF, PMI, log(share $word_i$ in group a/share $word_i$ in b), or create a measure similar to Ban's article. 

 # Q4 — Party as source of variation; which words discriminate each group?


```{r}
library(quanteda)

# build corpus and carry party labels as a docvar
df$party <- if ("party" %in% names(df)) df$party else df$party_clean
corp <- corpus(df, text_field = "text")

# tokens -> dfm (light trim)
toks <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE)
D    <- dfm(toks) |> dfm_trim(min_termfreq = 2)

# get group vector from docvars, keep only labeled rows
p <- docvars(D, "party")
keep <- !is.na(p) & nzchar(as.character(p))
D    <- D[keep, ]
p    <- p[keep]

# GROUP FIRST, then TF–IDF
Dby    <- dfm_group(D, groups = p)            # rows: party levels
Dtf_by <- dfm_tfidf(Dby, scheme_tf = "prop")

# top TF–IDF terms per party (prints)
feat  <- featnames(Dtf_by)
top_n <- 10
for (lev in rownames(Dtf_by)) {
  v <- as.numeric(Dtf_by[lev, ]); names(v) <- feat
  top <- head(sort(v, decreasing = TRUE), top_n)
  cat("\nTop", lev, "TF-IDF terms:\n")
  print(data.frame(term = names(top), tfidf = unname(top)), row.names = FALSE)
}
```

**thoughts and observations:**
I used party (Democrat vs. Republican) as the variation axis. After tokenizing, I grouped posts by party and computed TF-IDF, then took the top 10 tokens per group. Democrats’ top terms were mostly local/candidate tags (e.g., #swfl, county tags, banyai), while Republicans’ were national slogans/branding and handles (e.g., #takebackthehouse, #bidenflation, @jeffontheright). TF-IDF is clearly picking up party-specific campaign language; if I filter handles/URLs/hashtags and rerun, I’d expect more issue words to surface.

## Question 5

Create a dictionary (or use a pre-existing dictionary) to measure a topic of interest in your data. It can be anything, sentiment, tone, misinformation, any topic of interest. Label your documents, and visualize the prevalence of your classes. 

```{r}
library(stringr)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(quanteda.dictionaries)
library(quanteda.sentiment)

# read the csv file
df <- read.csv("/Users/zhaoshuti/Desktop/TAT/problem-set-01-2025-ShutiZhao/midterm_candidates_labeled_all_May05.csv")
glimpse(df)

# create a corpus
df_corpus <- corpus(df, text_field = "text")
docvars(df_corpus, "party_clean") <- df$party_clean

# make the tokens into a dfm
toks     <- tokens(df_corpus, remove_punct = TRUE)
dfm_all  <- dfm(toks, tolower = TRUE)
dfm_trim <- dfm_trim(dfm_all, min_termfreq = 2)

# lookup the dictionary 
dfm_sentiment <- quanteda::dfm_lookup(
  dfm_trim,
  dictionary = data_dictionary_LSD2015[c("positive","negative")]
)
```

```{r}
# per-doc counts -> add metadata -> majority label
sent_df <- dfm_sentiment |>
  quanteda::convert(to = "data.frame")

# make the first column name doc_id
names(sent_df)[1] <- "doc_id"

# ensure count columns exist
if (!"positive" %in% names(sent_df)) sent_df$positive <- 0L
if (!"negative" %in% names(sent_df)) sent_df$negative <- 0L

sent_df <- sent_df |>
  left_join(
    tibble(
      doc_id      = quanteda::docnames(dfm_trim),
      party_clean = quanteda::docvars(dfm_trim, "party_clean")
    ),
    by = "doc_id"
  ) |>
  mutate(
    positive = tidyr::replace_na(positive, 0L),
    negative = tidyr::replace_na(negative, 0L),
    major_sentiment = case_when(
      positive > negative ~ "Positive",
      negative > positive ~ "Negative",
      TRUE ~ "Neutral"
    )
  )
```

```{r}
# Show sentiment distribution
# overall prevalence
sent_df |>
  count(major_sentiment) |>
  ggplot(aes(major_sentiment, n, fill = major_sentiment)) +
  geom_col() +
  theme_minimal()

# by party (Dem vs Rep)
sent_df |>
  mutate(party2 = case_when(
    str_detect(party_clean, regex("dem", ignore_case = TRUE)) ~ "Democrat",
    str_detect(party_clean, regex("\\b(rep|gop)\\b", ignore_case = TRUE)) ~ "Republican",
    TRUE ~ "Other"
  )) |>
  filter(party2 %in% c("Democrat","Republican")) |>
  count(major_sentiment, party2) |>
  ggplot(aes(major_sentiment, n, fill = party2)) +
  geom_col(position = "stack") +
  theme_minimal()
```

**thoughts and observations:**
I measured sentiment with the LSD2015 dictionary (positive/negative). I tokenized, lowercased, removed punctuation, built a dfm, and counted dictionary hits. Each post got a Positive, Negative, or Neutral label by simple majority (ties → Neutral).

The prevalence plot shows a clear Positive > Neutral > Negative pattern overall. Splitting by party, both Democrats and Republicans follow the same ordering—campaign posts are mostly upbeat.

Limitations: bag-of-words and no context (e.g., negation, sarcasm). It’s a clean baseline; in Q6 I add a tiny negation tweak to handle phrases like “not support.”

## Question 6

Pick some documents (at least 10 for each class) that are exemplar (high probability) of being for each class of your dictionary. Read these documents.  Now let's try to augment a little your classifier.

```{r}
# make sure text is available
quanteda::docvars(dfm_trim, "text") <- df$text

docnames_vec <- quanteda::docnames(dfm_trim)
text_vec     <- quanteda::docvars(dfm_trim, "text")
party_vec    <- quanteda::docvars(dfm_trim, "party_clean")

# confidence margin
sent_df$margin <- abs(sent_df$positive - sent_df$negative)

pick_top <- function(df, label, k = 10) {
  sub <- df[df$major_sentiment == label, ]
  sub <- sub[order(-sub$margin, -sub$positive, sub$negative), ]
  sub <- head(sub, k)
  idx <- match(sub$doc_id, docnames_vec)
  sub$party   <- party_vec[idx]
  sub$text    <- text_vec[idx]
  sub$preview <- substr(sub$text, 1, 160)
  sub[, c("major_sentiment","party","doc_id","positive","negative","margin","preview")]
}

ex_pos_before <- pick_top(sent_df, "Positive", 10)
ex_neg_before <- pick_top(sent_df, "Negative", 10)

exemplars_before <- rbind(
  transform(ex_pos_before, class = "positive"),
  transform(ex_neg_before, class = "negative")
)

print(exemplars_before, row.names = FALSE)
```

```{r}
# add full text to exemplars (match doc_id -> dfm docs)
ex_pos_before$text <- quanteda::docvars(dfm_trim, "text")[match(ex_pos_before$doc_id,
                                                                quanteda::docnames(dfm_trim))]
ex_neg_before$text <- quanteda::docvars(dfm_trim, "text")[match(ex_neg_before$doc_id,
                                                                quanteda::docnames(dfm_trim))]
```

```{r}
# Treat "not/never/can't…" + (support/help/good/great/happy/proud/thank/win/love) as negative
neg_re <- "(?:\\bnot\\b|\\bno\\b|\\bnever\\b|isn['’]?t|wasn['’]?t|can['’]?t|won['’]?t|don['’]?t|doesn['’]?t|didn['’]?t|cannot)\\s+(support|help|good|great|happy|proud|thank|win|love)"

relabel_aug <- function(df) {
  t <- tolower(df$text)
  negated_pos <- stringr::str_count(t, neg_re)
  df$pos_aug  <- pmax(0L, df$positive - negated_pos)
  df$neg_aug  <- df$negative + negated_pos
  df$label_aug <- ifelse(df$pos_aug > df$neg_aug, "Positive",
                     ifelse(df$neg_aug > df$pos_aug, "Negative", "Neutral"))
  df
}

ex_after <- rbind(relabel_aug(ex_pos_before), relabel_aug(ex_neg_before))

# Show any flips
flips <- ex_after[ex_after$label_aug != ex_after$major_sentiment,
                  c("doc_id","major_sentiment","label_aug","positive","negative","pos_aug","neg_aug","preview")]
if (nrow(flips)) print(flips, row.names = FALSE)
```

```{r}
# top 10 exemplars per class
exemplars_before %>%
  dplyr::select(class, doc_id, preview) %>%
  dplyr::arrange(class) %>%
  dplyr::group_by(class) %>%
  dplyr::slice_head(n = 10) %>%
  print(n = 20)

# class counts before vs after
before_counts <- as.data.frame(table(sent_df$major_sentiment))
after_counts  <- as.data.frame(table(ex_after$label_aug))
cat("\nClass counts BEFORE:\n"); print(before_counts, row.names = FALSE)
cat("\nClass counts AFTER:\n");  print(after_counts,  row.names = FALSE)

# flips summary and a few examples
flips <- ex_after[ex_after$label_aug != ex_after$major_sentiment,
                  c("doc_id","major_sentiment","label_aug","preview")]
cat("\nFlips after augmentation:", nrow(flips), "\n")
if (nrow(flips) > 0) {
  print(utils::head(flips, 5), row.names = FALSE)
}
```


**thoughts and observations:**
I picked exemplars where only one side of the dictionary fired (positive>0 & negative=0, or negative>0 & positive=0) with total hits >5, then took 10 per class and read short previews. Negative exemplars were critiques/attack/problem posts (op-eds, complaints, issue contrasts). Positive exemplars were outreach/celebration posts—greetings, thanks, welcomes, community/event language. These match the dictionary’s intent.

Class balance (overall). Before augmentation the corpus skews positive: Positive ≈ 74k, Neutral ≈ 36k, Negative ≈ 24k—consistent with lots of thanking/announcements.

Augmentation & effect. I added a tiny negation rule (e.g., “not/never/can’t + support/help/good…” counts against positive). On the exemplar subset there were 0 flips—expected because they’re high-confidence, one-sided cases.

The dictionary behaves as intended on clear examples. The negation tweak is a safe improvement; next steps would be adding an alerts/disaster bucket (e.g., outage, emergency, rumor) and a few celebration terms (blessed, grateful, congrats), while treating ambiguous words like “support” as context-dependent.



### Question 6.1

Using cosine similarity, grab the closest 10 documents to each reference document you read before. Read those. How well does the cosine similarity work? Try with another measure of distance. Anything interesting there? 
```{r}
# load the reference documents
ref_docs <- head(docnames(dfm_trim), 3)

# keep raw text + a simple doc id in docvars to join later
docvars(dfm_trim, "text") <- df$text[seq_len(ndoc(dfm_trim))]
docvars(dfm_trim, "doc")  <- docnames(dfm_trim)

# build a tiny table with reference texts
sample_text <- tibble(
  doc_id = ref_docs,
  text   = docvars(dfm_trim, "text")[match(ref_docs, docnames(dfm_trim))]
)

# COSINE
closest_cosine <- textstat_simil(
  dfm_trim,
  y = dfm_trim[ref_docs, ],
  margin = "documents",
  method = "cosine"
) %>%
  as.data.frame() %>%
  group_by(document2) %>%
  arrange(desc(cosine), .by_group = TRUE) %>%
  slice_head(n = 10) %>%
  rename(
    doc     = document1,
    ref_doc = document2
  ) %>%
  # add neighbor text
  left_join(
    docvars(dfm_trim) %>% as.data.frame() %>% select(doc, text),
    by = "doc"
  ) %>%
  # add reference text
  left_join(
    sample_text %>% rename(ref_doc = doc_id, ref_text = text),
    by = "ref_doc"
  )

# peek 
closest_cosine %>%
  select(ref_doc, ref_text, doc, text, cosine) %>%
  arrange(ref_doc, desc(cosine)) %>%
  print(n = 30)
```

```{r}
# Correlation similarity
closest_cor <- textstat_simil(
  dfm_trim,
  y = dfm_trim[ref_docs, ],
  margin = "documents",
  method = "correlation"
) %>%
  as.data.frame() %>%
  group_by(document2) %>%
  arrange(desc(correlation), .by_group = TRUE) %>%
  slice_head(n = 10) %>%
  rename(
    doc     = document1,
    ref_doc = document2
  ) %>%
  left_join(
    docvars(dfm_trim) %>% as.data.frame() %>% select(doc, text),
    by = "doc"
  ) %>%
  left_join(
    sample_text %>% rename(ref_doc = doc_id, ref_text = text),
    by = "ref_doc"
  )

closest_cor %>%
  select(ref_doc, ref_text, doc, text, correlation) %>%
  arrange(ref_doc, desc(correlation)) %>%
  print(n = 30)
```

**thoughts and observations:**

Method. I used textstat_simil on my DFM to find the top-10 nearest neighbors for three reference posts (text1, text2, text3) with cosine similarity, then repeated the same with correlation and compared the lists.

Cosine – text2 (anniversary/thank-you). Scores are extremely high (two at ~0.910), and the neighbors look like near-duplicates with the same “Happy/blessed/thank you” phrasing. The rest (0.56–0.45) keep the same upbeat tone and vocabulary.

Cosine – text3 (Hurricane Ian update). Neighbors cluster tightly (~0.63–0.58), all around storm updates and public-info language. Cosine clearly groups posts that share concrete, event-specific terms.

Cosine – text1 (attack/critique). Scores are lower (~0.37–0.33) and neighbors are more mixed (generic “It’s/This…”, mentions). Less distinctive vocabulary makes it harder for cosine to find obviously “same-topic” matches.

Cosine vs. correlation. The correlation lists are almost the same as cosine (very similar scores and neighbors for all three refs). With sparse, non-negative counts, both measures rank documents similarly.

Takeaway. Cosine works well for near-duplicates and clearly topical posts; it’s less decisive for generic language. Correlation doesn’t add much beyond cosine here. If needed, TF-IDF or binary+Jaccard could sharpen neighbors for the generic case.

### Question 6.2

Now, check qualitative these documents. Take a look at the top features, keyword in Context, higher TF-IDF. Would you change anything in your dictionary now that you looked in these documents?

```{r}
# Top features (overall and by sentiment)
top_overall <- textstat_frequency(dfm_trim, n = 20)

groups_vec <- sent_df$major_sentiment[match(docnames(dfm_trim), sent_df$doc_id)]
top_by_sent <- textstat_frequency(dfm_trim, groups = groups_vec, n = 20)

# High TF-IDF terms by sentiment 
dfm_by        <- dfm_group(dfm_trim, groups = groups_vec)
dfm_tfidf_by  <- dfm_tfidf(dfm_by)

tfidf_top <- lapply(docnames(dfm_tfidf_by), function(g) {
  tibble(
    sentiment = g,
    term  = featnames(dfm_tfidf_by),
    tfidf = as.numeric(dfm_tfidf_by[g, ])
  ) |> arrange(desc(tfidf)) |> slice_head(n = 15)
}) |> bind_rows()

# Keywords-in-context 
toks_lc <- tokens_tolower(toks)
kw_terms <- c("inflation","crime","border","abortion","thank","support","hurricane")

kwic_tbl <- bind_rows(lapply(kw_terms, function(w) {
  k <- kwic(toks_lc, pattern = w, window = 5)
  if (!is.null(k) && nrow(k) > 0) as_tibble(k) |> mutate(keyword = w) else NULL
}))

# have a look
head(top_overall)
head(top_by_sent)
head(tfidf_top)
head(kwic_tbl)
```

**thoughts and observations:**

Top features. The most frequent words split between campaign language (thank/thanks, happy, congrats, blessed) and issue/event terms (inflation, crime, border, hurricane). By sentiment, Positive is heavy on celebration/thanks; Negative leans on critique/alert vocabulary.

KWIC. thank/thanks consistently express gratitude (clearly positive). inflation appears in complaint/critique contexts (clearly negative). hurricane shows urgent public-info updates—negative/alert tone, not hostile. support is mixed and context-dependent.

TF-IDF. High TF-IDF surfaces distinctive tokens that define posts: celebration terms (anniversary, blessed, congrats), event words (hurricane, ian, update), and sharp critique markers (fraud, rumor, outage). These cleanly separate upbeat personal posts from alerts/attacks.

Dictionary tweaks. Add to Positive: blessed, grateful, congrats, anniversary, appreciate, welcome. Add to Negative/Alert: shortage, outage, scam, rumor, fraud, disaster, emergency, evacuation. Treat support and generic campaign verbs as neutral/noise; consider multi-word phrases like “thank you” and “state of emergency.”

After reading examples, I’d expand positive celebration/thanks terms, add an alert/disaster bucket on the negative side, and neutralize ambiguous words. This should make the dictionary match how the posts actually read.